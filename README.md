"# DQN-Agent-F24" 

Build a DQN Agent to Successfully Land the Lunar Lander in OpenAI Gymâ€™s LunarLander-v2 Environment

Create a simple DQN epsilon policy network with 4 output neurons (one per possible action).  [Hint: DQN Agents use Epsilon greedy policy]        [15 points]

Discuss the rationale of the activation functions & the loss function used in the network. [10 points]

Define the hyperparameters: (i) the number of iterations, (ii) the number of episodes, (iii) the maximum number of steps, and (iv) the discount factor 
 at each step. [50 points]

Train the agent on the LunarLander-v2 environment for a sufficient number of episodes to achieve a satisfactory level of performance. [10 points]

Analyze the agent's learning progress by plotting relevant performance metrics (e.g., cumulative rewards, episode length) over time. [10 points]

Discuss the challenges faced during training and potential strategies for further improving the agent's performance. [5 points]